{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tweepy as tp\n",
    "from datetime import datetime, timedelta\n",
    "import regex as re\n",
    "\n",
    "bearerToken = \"AAAAAAAAAAAAAAAAAAAAAOUBZwEAAAAAuSI9Lk9VJF5p8oZ60%2Ffnb25FSXo%3DsH2SwTWEqpQOe0acAUZeAiPdazuwZYetImYMSn9Wzk7dmXR1VV\"\n",
    "\n",
    "client = tp.Client(bearer_token=bearerToken, wait_on_rate_limit=True)\n",
    "\n",
    "def convertDate(tweetObject):\n",
    "    tweetDate = tweetObject['created_at']\n",
    "    tweetDate = tweetDate.replace(\"T\", \" \")\n",
    "    tweetDate = str(tweetDate.split(\".\")[0])\n",
    "    tweetDate = datetime.strptime(tweetDate, '%Y-%m-%d %H:%M:%S')\n",
    "    return(tweetDate)\n",
    "\n",
    "\n",
    "\n",
    "def convertTime(inputDate):\n",
    "\n",
    "    inputDateSplit = inputDate.split(\"-\")\n",
    "\n",
    "    year = int(inputDateSplit[0])\n",
    "    month = int(inputDateSplit[1])\n",
    "    day = int(inputDateSplit[2])\n",
    "\n",
    "    if month < 10 and day < 10:\n",
    "        timeString = f\"{year}-0{month}-0{day}T00:00:00Z\"\n",
    "        return timeString\n",
    "    elif month < 10 and day >= 10:\n",
    "        timeString = f\"{year}-0{month}-{day}T00:00:00Z\"\n",
    "        return timeString\n",
    "    elif month >= 10 and day < 10:\n",
    "        timeString = f\"{year}-{month}-0{day}T00:00:00Z\"\n",
    "        return timeString\n",
    "    else:\n",
    "        timeString = f\"{year}-{month}-{day}T00:00:00Z\"\n",
    "        return timeString\n",
    "\n",
    "def generateTweets(date, df, brand):\n",
    "\n",
    "    tweets = client.search_recent_tweets(query=brand, tweet_fields=['context_annotations', 'created_at', 'text', 'author_id', 'source', 'public_metrics'], max_results=100, end_time=date)\n",
    "        \n",
    "    for tweet in tweets.data:\n",
    "        tweet_ID = tweet.id\n",
    "        tweet_content = tweet.text\n",
    "        tweet_date = tweet.created_at\n",
    "        tweet_user = tweet.user\n",
    "        tweet_retweets = tweet.public_metrics['retweet_count']\n",
    "        tweet_replies = tweet.public_metrics['reply_count']\n",
    "        tweet_likes = tweet.public_metrics['like_count']\n",
    "\n",
    "        tweet_quotes = tweet.public_metrics['quote_count']\n",
    "\n",
    "        df = df.append({'TweetID': str(tweet_ID), 'Content':str(tweet_content), 'Full_Date':tweet_date,\n",
    "                'Retweets':tweet_retweets, 'Replies':tweet_replies, 'Likes':tweet_likes, 'Quotes':tweet_quotes}, ignore_index=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't work: \"Love the new\", \"must try the\"\n",
    "#Potential: favorite snack, favorite drink, love drink, love snack (without quotes)\n",
    "#Works: \"Tried the new\", flavor good\n",
    "phrases = ['flavor good', \"Tried the new\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(columns=['TweetID', 'Content', 'Full_Date', 'Retweets', 'Replies', 'Likes', 'Quotes'])\n",
    "for phrase in phrases:\n",
    "    brand = f'{phrase} -is:retweet'\n",
    "            \n",
    "\n",
    "    for i in range(6):\n",
    "        date = convertTime((datetime.now() - timedelta(days=i)).strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        df = generateTweets(date, df, brand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_excel(\"phraseAnalyse.xlsx\")\n",
    "\n",
    "df['created_dt'] = df.Full_Date.dt.date\n",
    "df['created_time'] = df.Full_Date.dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetID</th>\n",
       "      <th>Content</th>\n",
       "      <th>Full_Date</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Replies</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Quotes</th>\n",
       "      <th>Only_Date</th>\n",
       "      <th>Only_Time</th>\n",
       "      <th>tweet_len</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>link_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>uppercase_count</th>\n",
       "      <th>numeric_count</th>\n",
       "      <th>mention_tags</th>\n",
       "      <th>created_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1523814395796676609</td>\n",
       "      <td>@owenyaoi WHATâ€™S ADV.... BUT OMG I HOPE EVERYT...</td>\n",
       "      <td>2022-05-09 23:57:17+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-05-09</td>\n",
       "      <td>23:57:17</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>[@owenyaoi]</td>\n",
       "      <td>2022-05-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1523813828147830784</td>\n",
       "      <td>@PerfumeMountain @HourComeAtLast @ProperDrashi...</td>\n",
       "      <td>2022-05-09 23:55:02+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-05-09</td>\n",
       "      <td>23:55:02</td>\n",
       "      <td>153</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[@PerfumeMountain, @HourComeAtLast, @ProperDra...</td>\n",
       "      <td>2022-05-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1523812904805445632</td>\n",
       "      <td>@AmericanEDMGirl Yeah they were good!, birthda...</td>\n",
       "      <td>2022-05-09 23:51:22+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-05-09</td>\n",
       "      <td>23:51:22</td>\n",
       "      <td>95</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[@AmericanEDMGirl]</td>\n",
       "      <td>2022-05-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1523812265375457280</td>\n",
       "      <td>@MonkMan2015 Really good my guy. Loads of flav...</td>\n",
       "      <td>2022-05-09 23:48:49+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-05-09</td>\n",
       "      <td>23:48:49</td>\n",
       "      <td>99</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[@MonkMan2015]</td>\n",
       "      <td>2022-05-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1523810349820096512</td>\n",
       "      <td>@QuestNutrition Thanks for the reply. Just did...</td>\n",
       "      <td>2022-05-09 23:41:13+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-05-09</td>\n",
       "      <td>23:41:13</td>\n",
       "      <td>165</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[@QuestNutrition]</td>\n",
       "      <td>2022-05-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>1521979927649734657</td>\n",
       "      <td>Tried the new Mushroom Jack Burger at @RoysRes...</td>\n",
       "      <td>2022-05-04 22:27:46+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>22:27:46</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[@RoysRestaurants]</td>\n",
       "      <td>2022-05-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>1521979651169660931</td>\n",
       "      <td>Tried this new thing at the gym and i must hav...</td>\n",
       "      <td>2022-05-04 22:26:40+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>22:26:40</td>\n",
       "      <td>177</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>2022-05-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>1521979176756068352</td>\n",
       "      <td>Kanye told yâ€™all this is what happened,  but y...</td>\n",
       "      <td>2022-05-04 22:24:47+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>22:24:47</td>\n",
       "      <td>304</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>2022-05-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>1521979050708901892</td>\n",
       "      <td>@progressive I received an email meant for a c...</td>\n",
       "      <td>2022-05-04 22:24:17+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>22:24:17</td>\n",
       "      <td>273</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[@progressive]</td>\n",
       "      <td>2022-05-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>1521978706423685121</td>\n",
       "      <td>#DoctorStrange \\nWhere a movie like The Eterna...</td>\n",
       "      <td>2022-05-04 22:22:55+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>22:22:55</td>\n",
       "      <td>270</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>2022-05-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1193 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  TweetID                                            Content  \\\n",
       "0     1523814395796676609  @owenyaoi WHATâ€™S ADV.... BUT OMG I HOPE EVERYT...   \n",
       "1     1523813828147830784  @PerfumeMountain @HourComeAtLast @ProperDrashi...   \n",
       "2     1523812904805445632  @AmericanEDMGirl Yeah they were good!, birthda...   \n",
       "3     1523812265375457280  @MonkMan2015 Really good my guy. Loads of flav...   \n",
       "4     1523810349820096512  @QuestNutrition Thanks for the reply. Just did...   \n",
       "...                   ...                                                ...   \n",
       "1188  1521979927649734657  Tried the new Mushroom Jack Burger at @RoysRes...   \n",
       "1189  1521979651169660931  Tried this new thing at the gym and i must hav...   \n",
       "1190  1521979176756068352  Kanye told yâ€™all this is what happened,  but y...   \n",
       "1191  1521979050708901892  @progressive I received an email meant for a c...   \n",
       "1192  1521978706423685121  #DoctorStrange \\nWhere a movie like The Eterna...   \n",
       "\n",
       "                     Full_Date Retweets Replies Likes Quotes   Only_Date  \\\n",
       "0    2022-05-09 23:57:17+00:00        0       1     0      0  2022-05-09   \n",
       "1    2022-05-09 23:55:02+00:00        0       1     2      0  2022-05-09   \n",
       "2    2022-05-09 23:51:22+00:00        0       0     1      0  2022-05-09   \n",
       "3    2022-05-09 23:48:49+00:00        0       1     2      0  2022-05-09   \n",
       "4    2022-05-09 23:41:13+00:00        0       0     0      0  2022-05-09   \n",
       "...                        ...      ...     ...   ...    ...         ...   \n",
       "1188 2022-05-04 22:27:46+00:00        0       1     5      0  2022-05-04   \n",
       "1189 2022-05-04 22:26:40+00:00        0       1     0      0  2022-05-04   \n",
       "1190 2022-05-04 22:24:47+00:00        2       1     2      0  2022-05-04   \n",
       "1191 2022-05-04 22:24:17+00:00        0       1     0      0  2022-05-04   \n",
       "1192 2022-05-04 22:22:55+00:00        0       1     0      0  2022-05-04   \n",
       "\n",
       "     Only_Time  tweet_len  stopword_count  hashtag_count  link_count  \\\n",
       "0     23:57:17        136               1              0           0   \n",
       "1     23:55:02        153               7              0           0   \n",
       "2     23:51:22         95               6              0           0   \n",
       "3     23:48:49         99               5              0           0   \n",
       "4     23:41:13        165              11              0           0   \n",
       "...        ...        ...             ...            ...         ...   \n",
       "1188  22:27:46        103               3              0           1   \n",
       "1189  22:26:40        177              16              0           0   \n",
       "1190  22:24:47        304              20              0           2   \n",
       "1191  22:24:17        273              17              0           0   \n",
       "1192  22:22:55        270              21              1           0   \n",
       "\n",
       "      mention_count  uppercase_count  numeric_count  \\\n",
       "0                 1               21              0   \n",
       "1                 4                0              0   \n",
       "2                 1                2              0   \n",
       "3                 1                0              0   \n",
       "4                 1                0              0   \n",
       "...             ...              ...            ...   \n",
       "1188              1                0              0   \n",
       "1189              0                0              0   \n",
       "1190              0                0              0   \n",
       "1191              1                2              0   \n",
       "1192              0                2              1   \n",
       "\n",
       "                                           mention_tags  created_dt  \n",
       "0                                           [@owenyaoi]  2022-05-09  \n",
       "1     [@PerfumeMountain, @HourComeAtLast, @ProperDra...  2022-05-09  \n",
       "2                                    [@AmericanEDMGirl]  2022-05-09  \n",
       "3                                        [@MonkMan2015]  2022-05-09  \n",
       "4                                     [@QuestNutrition]  2022-05-09  \n",
       "...                                                 ...         ...  \n",
       "1188                                 [@RoysRestaurants]  2022-05-04  \n",
       "1189                                                 []  2022-05-04  \n",
       "1190                                                 []  2022-05-04  \n",
       "1191                                     [@progressive]  2022-05-04  \n",
       "1192                                                 []  2022-05-04  \n",
       "\n",
       "[1193 rows x 18 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vetlefrantzvaag/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vetlefrantzvaag/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "# Load dependencies\n",
    "#----------------------------------------------\n",
    "import pandas as pd\n",
    "import base64\n",
    "import tweepy as tw\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "import gensim\n",
    "import pyLDAvis.gensim_models\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.corpora as corpora\n",
    "import streamlit as st\n",
    "from pprint import pprint\n",
    "from nltk.util import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from streamlit_metrics import metric, metric_row\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "# DEFINE STOPWORDS\n",
    "#----------------------------------------------\n",
    "\n",
    "# English stopwords\n",
    "stopwords_en = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1\n",
    "# Downloading the .csv from the dashboard\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~\n",
    "def get_table_download_link(df):\n",
    "    # Reference: https://discuss.streamlit.io/t/how-to-download-file-in-streamlit/1806\n",
    "    \"\"\"\n",
    "    Generates a link allowing the data in a given panda dataframe to be downloaded\n",
    "    in:  dataframe\n",
    "    out: href string\n",
    "    \"\"\"\n",
    "    csv = df.to_csv(index=False)\n",
    "    b64 = base64.b64encode(csv.encode()).decode()  # some strings <-> bytes conversions necessary here\n",
    "    href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"tweets.csv\">Download CSV file</a>'\n",
    "    return href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 2\n",
    "# Feature extraction (will be useful for further analysis)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~\n",
    "def feature_extraction(df):\n",
    "    # Extracting the date and time\n",
    "    df['Only_Date'] = df.Full_Date.dt.date\n",
    "    df['Only_Time'] = df.Full_Date.dt.time\n",
    "\n",
    "    #Length of tweet\n",
    "    df['tweet_len'] = df.Content.apply(lambda x: len(x))\n",
    "\n",
    "    #Stopword count\n",
    "    df['stopword_count'] = df.Content.apply(lambda x: len([x for x in x.split() if x in stopwords_en]))\n",
    "\n",
    "    #Hashtag count\n",
    "    df['hashtag_count'] = df.Content.apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "    \n",
    "    #Hashtag count\n",
    "    df['link_count'] = df.Content.apply(lambda x: len([x for x in x.split() if x.startswith('https')]))\n",
    "\n",
    "    #Mention count\n",
    "    df['mention_count'] = df.Content.apply(lambda x: len([x for x in x.split() if x.startswith('@')]))\n",
    "\n",
    "    #Mention tags\n",
    "    df['mention_tags'] = df.Content.apply(lambda x: [x for x in x.split() if x.startswith('@')])\n",
    "\n",
    "    #Numeric count\n",
    "    df['numeric_count'] = df.Content.apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "\n",
    "    #Numeric count\n",
    "    df['uppercase_count'] = df.Content.apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the most mentioned accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "countList = []\n",
    "for mention in df.mention_tags:\n",
    "    for tag in mention:\n",
    "        countList.append(tag)\n",
    "Counter(countList).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 4 (a)\n",
    "# Cleaning\n",
    "#-------------\n",
    "def round1_text_clean(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text) # remove emoji\n",
    "    text = ' ' + text # added space because there was some weirdness for first word (strip later)\n",
    "    text = text.lower() # convert all text to lowercase\n",
    "    text = re.sub(r'(\\s)@\\w+', '', text) # remove whole word if starts with @\n",
    "    text = re.sub(r'(\\s)\\w*\\d\\w*\\w+', '', text) # remove whole word if starts with number\n",
    "    text = re.sub(r'https\\:\\/\\/t\\.co\\/*\\w*', '', text) # remove https links\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # removes punctuation\n",
    "    text = re.sub('\\[.*?\\]', '', text) # removes text in square brackets\n",
    "    #text = re.sub('\\w*\\d\\w*', '', text) # remove whole word if starts with number\n",
    "    #text = re.sub(r'(\\s)#\\w+', '', text) # remove whole word if starts with #\n",
    "    text = text.strip() # strip text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 4b\n",
    "#-------------\n",
    "text_clean_round1 = lambda x: round1_text_clean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 5\n",
    "#-------------\n",
    "def text_clean_round2(text):\n",
    "    \"\"\"\n",
    "    A simple function to clean up the data. All the words that\n",
    "    are not designated as a stop word is then lemmatized after\n",
    "    encoding and basic regex parsing are performed.\n",
    "    \"\"\"\n",
    "    nltk.download('wordnet')\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "    .encode('ascii', 'ignore')\n",
    "    .decode('utf-8', 'ignore'))\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96defd4141cd10c7eaaddd0ac829687b13c50513df206399231bf58d1c0a2231"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
